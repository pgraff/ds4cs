{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b433cd-70ac-42a0-98d9-22e18babbf4b",
   "metadata": {},
   "source": [
    "# Tensorflow MNIST\n",
    "\n",
    "## Introducting the dataset\n",
    "\n",
    "The MNIST database is a set of handwritten digits captured and open sourced. \n",
    "The images have been labeled and makes for an excellent source for testing out a neural network.\n",
    "\n",
    "You can read more about the dataset here:\n",
    "https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "Below are some sample of the images\n",
    "<img src=\"img/MnistExamples.png\">\n",
    "\n",
    "Training on the MNIST dataset is often talked about as the __hello world__ of machine learning.\n",
    "\n",
    "## The plan\n",
    "\n",
    "We will build a neural network using Tensorflow. The network will look something like this:\n",
    "\n",
    "<img src=\"img/mnist_2layers.png\">\n",
    "\n",
    "We'll have a 784 (28x28) input layer and a 10 neuron output layer (one neuron for each digit). In addition, we'll use one hidden layer.\n",
    "\n",
    "We will use Tensorflow to construct the neural net.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb0d69-e0da-4238-baf9-87000d6992e0",
   "metadata": {},
   "source": [
    "## Install Tensorflow (if not already present)\n",
    "\n",
    "First, let's make sure that we have tensorflow installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29bfda-aea2-4dc0-a1e9-8a6d5d1c66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%!\n",
    "pip install tensorflow\n",
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689739b-dabf-4b66-be8f-612d89590fcd",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We have a few libraries that we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21f1b5-f79c-4a8b-97de-69ab13d71a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2738dd-a0fe-44f9-abc6-c44701b544ae",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The good news for this exercise is that we can load the data using the imports.\n",
    "\n",
    "The data is split into two sets.\n",
    "\n",
    "* `ds_train` is the set that we'll use to train our model\n",
    "* `ds_test` we will use to measure how well our model is doing\n",
    "\n",
    "The arguuments:\n",
    "\n",
    "* `shuffle_files=True`: The MNIST data is only stored in a single file, but for larger datasets with multiple files on disk, it's good practice to shuffle them when training.\n",
    "* `as_supervised=True`: Returns a tuple (img, label) instead of a dictionary {'image': img, 'label': label}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0971cc9-da6a-49de-9911-5d349664dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7172e-d30e-486e-bd46-f3428404d08f",
   "metadata": {},
   "source": [
    "## Checking the data set\n",
    "\n",
    "Let's take a look at the size of each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5adfc-584b-4a90-9b2b-067cd0640149",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training set:\", ds_train.cardinality().numpy())\n",
    "print(\"Size of test set.   :\", ds_test.cardinality().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a590b64-93c1-4e5d-9a13-0b7df923ae02",
   "metadata": {},
   "source": [
    "## Define a normalization function\n",
    "\n",
    "The image is using integers, we need floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c06817-7c96-4650-a963-4b155860d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3595f-c35f-4f58-a55f-90d3841ee99d",
   "metadata": {},
   "source": [
    "## Setup training pipeline\n",
    "\n",
    "Next, we have to build a training pipeline\n",
    "\n",
    "1. TFDS provides os images where each pixel is exppressed as an integer (or a `tf.uint8`). For that reason, we defined the `normalize_image` function. We now have to apply this function.\n",
    "2. `tf.data.Dataset.cache`: As you fit the dataset in memory, cache it before shuffling for a better performance.\n",
    "3. `tf.data.Dataset.shuffle`: For true randomness, set the shuffle buffer to the full dataset size.\n",
    "4. `tf.data.Dataset.batch`: Batch elements of the dataset after shuffling to get unique batches at each epoch.\n",
    "5. `tf.data.Dataset.prefetch`: It is good practice to end the pipeline by prefetching for performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a47af-90f8-4b26-aa9e-1cb02b4a9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820d3a9-c8e6-47d3-a042-ff9aa9229023",
   "metadata": {},
   "source": [
    "\n",
    "## Build an evaluation pipeline\n",
    "\n",
    "Your testing pipeline is similar to the training pipeline with small differences:\n",
    "\n",
    "You obviously don't need to call shuffle (it doesn't matter in what order we test).\n",
    "Caching is done after batching because batches can be the same between epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad921e7-ec81-4be3-85bf-df23550a7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3a7b6-f3be-4e19-9f82-786f1318bf09",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Plug the TFDS input pipeline into a simple Keras model, compile the model, and train it.\n",
    "\n",
    "Notice a few things:\n",
    "\n",
    "1. We are specifying the input shape to be 28 by 28 (size of the image)\n",
    "2. We define one intermediate layer using 128 nodes (or neurons) with the activation function `relu`\n",
    "3. The final output layer is of size 10 (obviously, as we're trying to see which single digit we believe the image matches)\n",
    "4. We use the optimizer (or optimization function) called Adam\n",
    "5. We use a loss function called `SpaseCategoricalAccuracy`\n",
    "6. We use 6 epochs (An epoch is when all the training data is used at once and is defined as the total number of iterations of all the training data in one cycle for training the machine learning model. Another way to define an epoch is the number of passes a training dataset takes around an algorithm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891edda-5bb5-449d-bef3-9e470d101d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96fdb5-fac9-46f0-b3dc-c95c2d878f6d",
   "metadata": {},
   "source": [
    "You should now (in theory at least) see the accuracy increasing with each epoch. I would expect ~98% accuracy.\n",
    "\n",
    "You could increase the number of epochs to improve the accuracy. When I ran the experiment, I got to 1.0 accuracy after 35 epochs, but my suspicion is that the model is overfit at that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d02e9-34a2-4030-8a8c-1f2962143bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
